{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook explores two fine tuned models to support Artisse AI Prompts\n",
        "\n",
        "*   The first model is finetuned GPT2\n",
        "*   The second model is finetuned GPT Neo\n",
        "*   These models were originall trained on Stability AI prompts\n",
        "*   This is a draft. The prompts the models output have not been tested against Artisse AI's actual image generation algorithm\n",
        "\n",
        "## To run the\n",
        "\n",
        "\n",
        "1.  Selectect \"Runtime\" from the top menu of this Google Colab\n",
        "2.  Select \"Run All\"\n",
        "3.  You will be prompted twice: 1st to allow this notebook to run in your Google Drive. Then to allow this notebook to download the models into your Google Drive.\n",
        "4.  It will take some time to download the models\n",
        "5.  Scroll to the bottom of the notebook and explore the applet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BGWl9q6amjnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HRkgZFaQSJ-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "from google.colab import drive\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW, get_linear_schedule_with_warmup, AutoModelForTokenClassification, TokenClassificationPipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn off logging:\n",
        "import logging\n",
        "\n",
        "# Disable logging\n",
        "logging.disable(logging.CRITICAL)\n"
      ],
      "metadata": {
        "id": "HgBKM5CVldIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the models\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import os\n",
        "\n",
        "# Authenticate and mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with Google Drive API\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the IDs and paths for the models\n",
        "model_id_v1 = \"1sLoRP8tt4H7eHM-d-KS-rySsu0E_CY4U\"\n",
        "tokenizer_id_v1 = \"1nS5vQY65WfLl71w3Un1wOl8ccKHIR9Ho\"\n",
        "\n",
        "model_id_v2 = \"1LAAkIiBFgVMJP2EQGutEUVez5Oaz_eYU\"\n",
        "tokenizer_id_v2 = \"1t1stRthDWMRi46tt4ko9AwF_sQYL8fnj\"\n",
        "\n",
        "folder_path_model = [\n",
        "    \"/content/drive/MyDrive/artisse_models/v1/model\",\n",
        "    \"/content/drive/MyDrive/artisse_models/v1/tokenizer\",\n",
        "\n",
        "    \"/content/drive/MyDrive/artisse_models/v2/model\",\n",
        "    \"/content/drive/MyDrive/artisse_models/v2/tokenizer\"\n",
        "]\n",
        "\n",
        "# Create the folders if they don't exist\n",
        "for f in folder_path_model:\n",
        "    if not os.path.exists(f):\n",
        "        os.makedirs(f)\n",
        "        print(\"Folder created successfully!\")\n",
        "    else:\n",
        "        print(\"Folder already exists.\")\n",
        "\n",
        "# Download the model and tokenizer files\n",
        "for model_folder, file_id in zip(folder_path_model, [model_id_v1, tokenizer_id_v1, model_id_v2, tokenizer_id_v2]):\n",
        "    # List all files in the folder\n",
        "    file_list = drive_service.files().list(q=\"'{}' in parents and trashed=false\".format(file_id), spaces='drive',\n",
        "                                           fields='files(id, name)').execute().get('files', [])\n",
        "\n",
        "    # Download each file\n",
        "    for file in file_list:\n",
        "        file_path = os.path.join(model_folder, file['name'])\n",
        "        request = drive_service.files().get_media(fileId=file['id'])\n",
        "        fh = io.FileIO(file_path, mode='wb')\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "            print(f\"Downloaded {int(status.progress() * 100)}% of {file['name']}\")\n"
      ],
      "metadata": {
        "id": "fbXo4CFwaf1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chwuDUXeWRgA"
      },
      "outputs": [],
      "source": [
        "# Load the model from your Google Drive\n",
        "model_path_v1 = '/content/drive/MyDrive/artisse_models/v1/model'\n",
        "tokenizer_path_v1 = '/content/drive/MyDrive/artisse_models/v1/tokenizer'\n",
        "model_path_v2 = '/content/drive/MyDrive/artisse_models/v2/model'\n",
        "tokenizer_path_v2 = '/content/drive/MyDrive/artisse_models/v2/tokenizer'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_v1 = AutoModelForCausalLM.from_pretrained(model_path_v1)\n",
        "tokenizer_v1 = AutoTokenizer.from_pretrained(tokenizer_path_v1)\n",
        "\n",
        "model_v2 = AutoModelForCausalLM.from_pretrained(model_path_v2)\n",
        "tokenizer_v2 = AutoTokenizer.from_pretrained(tokenizer_path_v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-319GWkUaAw"
      },
      "outputs": [],
      "source": [
        "# Load models for verb removal\n",
        "# https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english?text=My+name+is+Sarah+and+I+live+in+London\n",
        "model_pos_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
        "tokenizer_pos = AutoTokenizer.from_pretrained(model_pos_name)\n",
        "model_pos = AutoModelForTokenClassification.from_pretrained(model_pos_name)\n",
        "pipeline = TokenClassificationPipeline(model=model_pos, tokenizer=tokenizer_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nr0pHZUVMKL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cleanup_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Cleans up a sentence by removing leading/trailing spaces, capitalizing the first letter,\n",
        "    fixing word tokenization issues, and removing extra spaces before punctuation marks.\n",
        "\n",
        "    Parameters:\n",
        "    - sentence: A string representing the sentence to be cleaned up.\n",
        "\n",
        "    Return Value:\n",
        "    - cleaned_sentence: A string containing the cleaned-up sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove leading/trailing spaces\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # Capitalize the first letter\n",
        "    sentence = sentence.capitalize()\n",
        "\n",
        "    # Fix word tokenization issues\n",
        "    # Combine replacements\n",
        "    sentence = sentence.replace(\" ##\", \"\").replace(\" ' \", \"'\")\n",
        "\n",
        "    # Remove extra spaces before punctuation marks\n",
        "    sentence = re.sub(r'\\s*([.,?!:])', r'\\1', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def reconstruct_sentence(orig_sentence, pos):\n",
        "    \"\"\"\n",
        "    Reconstructs a sentence by filtering out verbs, adverbs, and subordinating conjunctions based on their part-of-speech tags.\n",
        "    The reconstructed sentence is then cleaned up using the cleanup_sentence() function.\n",
        "\n",
        "    Parameters:\n",
        "    - orig_sentence: An array of the original sentence with punctuation still connected to words.\n",
        "    - pos: A list of dictionaries containing information about each word's part-of-speech.\n",
        "\n",
        "    Return Value:\n",
        "    - reconstructed_text: A string containing the reconstructed and cleaned-up sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_words = []\n",
        "    for word_info in pos:\n",
        "        index = word_info['index'] - 1  # Adjust index to match Python's 0-based indexing\n",
        "        if word_info['entity'] != 'VRB' and word_info['entity'] != 'VBG' and word_info['entity'] != 'RB':\n",
        "            word = pos[index]['word']\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    reconstructed_text = cleanup_sentence(' '.join(filtered_words))\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "\n",
        "def prompt_editor(original_prompt):\n",
        "    \"\"\"\n",
        "    Edits a prompt by splitting it into words, processing it using an NLP pipeline, and then reconstructing and cleaning up the sentence.\n",
        "\n",
        "    Parameters:\n",
        "    - original_prompt: A string representing the original prompt to be edited.\n",
        "\n",
        "    Return Value:\n",
        "    - edited_prompt: A string containing the edited and cleaned-up prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    orig_word_array = original_prompt.split(' ')\n",
        "    outputs = pipeline(original_prompt)  # Assuming pipeline is defined elsewhere\n",
        "    return reconstruct_sentence(orig_word_array, outputs)\n",
        "\n",
        "# prompt_editor(\"A stylish lady in a sleek business suit, confidently leading a meeting in a modern office space in Asia.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSPJDbefZJxv"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"A petite lady in a vibrant floral dress, lafing heartily while enjoyin a cup of cofee in a cuaint café in Rome.\"\n",
        "# poorly_written_prompt= \"A woman in a chef's apron, preparing a meal in a sunlit kitchen.\"\n",
        "# poorly_written_prompt = \"A petite lady in a vibrant floral dress, lafing heartily while enjoyin a cup of cofee in a cuaint café in Rome.\"\n",
        "\n",
        "# poorly_written_prompt= \"A woman in a ski outfit, drinking hot cocoa, and holding hands with her mini dog\"\n",
        "# poorly_written_prompt= \"A man in a ski outfit, drinking hot cocoa, and with puffs of snow on his hair\"\n",
        "prompt2 = \"a man in a garden, sunshine on his face, eating an almond\"\n",
        "# Generate the attention mask\n",
        "\n",
        "encoded_prompt = tokenizer_v1.encode(prompt2,  return_tensors=\"pt\")\n",
        "encoded_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_GyONbmXke6"
      },
      "outputs": [],
      "source": [
        "attention_mask = torch.ones(encoded_prompt.shape, dtype=torch.long, device=encoded_prompt.device)\n",
        "\n",
        "\n",
        "output = model_v1.generate(encoded_prompt, max_length=50, attention_mask=attention_mask, num_return_sequences=1)\n",
        "improved_prompt = tokenizer_v1.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Improved prompt:\", improved_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKyu9J2YX0VA"
      },
      "outputs": [],
      "source": [
        "# top - k  top - p combined\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "\n",
        "attention_mask = torch.ones(encoded_prompt.shape, dtype=torch.long, device=encoded_prompt.device)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model_v1.generate(\n",
        "    encoded_prompt,\n",
        "    do_sample=True,\n",
        "    max_length=40,\n",
        "    top_k=50,\n",
        "    attention_mask = attention_mask, #torch.ones(encoded_prompt.shape, dtype=torch.long, device=encoded_prompt.device)\n",
        "    top_p=0.98,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer_v1.decode(sample_output, skip_special_tokens=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6TjrQt7e5Py"
      },
      "outputs": [],
      "source": [
        "# top-k top-p combined\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "def generate_prompt(original_prompt, model, tokenizer):\n",
        "    encoded_prompt = tokenizer.encode(original_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    attention_mask = torch.ones(encoded_prompt.shape, dtype=torch.long, device=encoded_prompt.device)\n",
        "\n",
        "    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "    sample_outputs = model.generate(\n",
        "        encoded_prompt,\n",
        "        do_sample=True,\n",
        "        max_length=60,\n",
        "        top_k=50,\n",
        "        attention_mask=attention_mask,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=3\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# test\n",
        "# generate_prompt(\"a lady\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applet\n",
        "Scroll to bottom of notebook. In the applet that appears, type in prompts, press Generate and read the completed and edited prompt"
      ],
      "metadata": {
        "id": "z7eCYyHJnqpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "FWOLld19LS3S",
        "outputId": "4f9fdcd4-790c-431f-f6dd-e17861710cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7864, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "def initiate_model_gpt2(original_prompt):\n",
        "    \"\"\"\n",
        "    Initiates the model by generating and editing a prompt based on the given original_prompt.\n",
        "\n",
        "    Args:\n",
        "        original_prompt (str): The original prompt to start the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The edited prompt generated by the model.\n",
        "\n",
        "    \"\"\"\n",
        "    generated_prompt = generate_prompt(original_prompt, model_v1, tokenizer_v1)\n",
        "    edited_prompt = prompt_editor(generated_prompt)\n",
        "    return edited_prompt\n",
        "\n",
        "def initiate_model_neo(original_prompt):\n",
        "    \"\"\"\n",
        "    Initiates the model by generating and editing a prompt based on the given original_prompt.\n",
        "\n",
        "    Args:\n",
        "        original_prompt (str): The original prompt to start the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The edited prompt generated by the model.\n",
        "\n",
        "    \"\"\"\n",
        "    generated_prompt = generate_prompt(original_prompt, model_v2, tokenizer_v2)\n",
        "    edited_prompt = prompt_editor(generated_prompt)\n",
        "    return edited_prompt\n",
        "\n",
        "# def add_example_to_input_gpt2(example):\n",
        "#     text_input_gpt2.value += example\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Complete your prompt using a custom trained model (verbs will automatically be removed)\")\n",
        "\n",
        "    with gr.Accordion(\"Examples of prompts\") as accordion:\n",
        "        accordion = gr.Markdown(\"A focused lady in a red bodysuit, laughing heartily while enjoying a cup of coffee in a quaint café in Rome.<br>A man in a stylish tuxedo, strolling through a tranquil park in Chengdu.\")\n",
        "\n",
        "    with gr.Tab(\"initiate fine-tuned GPT2\"):\n",
        "        text_input_gpt2 = gr.Textbox(label=\"Original Prompt\", placeholder=\"Write a prompt for image generation\")\n",
        "        text_output_gpt2 = gr.Textbox(label=\"Generated Prompt with fine-tuned GPT2\")\n",
        "        text_button2 = gr.Button(\"Complete Prompt\")\n",
        "    with gr.Tab(\"initiate fine-tuned GPT Neo\"):\n",
        "        text_input_neo = gr.Textbox(label=\"Original Prompt\", placeholder=\"Write a prompt for image generation\")\n",
        "        text_output_neo = gr.Textbox(label=\"Generated Prompt with fine-tuned GPT Neo\")\n",
        "        text_button3 = gr.Button(\"Complete Prompt\")\n",
        "\n",
        "\n",
        "    text_button2.click(initiate_model_gpt2, inputs=text_input_gpt2, outputs=text_output_gpt2)\n",
        "    text_button3.click(initiate_model_neo, inputs=text_input_neo, outputs=text_output_neo)\n",
        "\n",
        "\n",
        "demo.launch(debug=False) # set the to true to show errors in gradio\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BGWl9q6amjnS"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}